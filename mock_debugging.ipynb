{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cdf54b",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "Scenario: \"You join a users dataframe (1M rows) with a transactions dataframe (10M rows) on user_id. Your resulting dataframe has 50M rows. Why might this be happening, and how would you debug it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7934744",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "Scenario: \"You join a users dataframe (1M rows) with a transactions dataframe (10M rows) on user_id. Your resulting dataframe has 50M rows. Why might this be happening, and how would you debug it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c406b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/26 11:32:38 WARN Utils: Your hostname, Jiaxins-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.23 instead (on interface en0)\n",
      "25/10/26 11:32:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/26 11:32:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/26 11:32:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users count: 100\n",
      "Transactions count: 100\n",
      "Joined count: 2000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "import random\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"JoinExplosionDebug\").getOrCreate()\n",
    "\n",
    "# Create Users DataFrame (with intentional duplicates)\n",
    "users_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"Diana\"),\n",
    "    (5, \"Evan\"),\n",
    "]\n",
    "\n",
    "# Simulate duplicates — some user_ids appear twice\n",
    "users_data *= 20  # total 100 rows\n",
    "users_df = spark.createDataFrame(users_data, [\"user_id\", \"user_name\"])\n",
    "\n",
    "# Create Transactions DataFrame (multiple transactions per user)\n",
    "transactions_data = []\n",
    "for i in range(100):\n",
    "    user_id = random.choice([1, 2, 3, 4, 5])\n",
    "    transactions_data.append((i, user_id, random.randint(10, 500)))\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"transaction_id\", \"user_id\", \"amount\"])\n",
    "\n",
    "# Perform join\n",
    "joined_df = users_df.join(transactions_df, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "print(\"Users count:\", users_df.count())\n",
    "print(\"Transactions count:\", transactions_df.count())\n",
    "print(\"Joined count:\", joined_df.count())  # Expect this to 'explode'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cebf4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      "\n",
      "+-------+------------------+---------+\n",
      "|summary|           user_id|user_name|\n",
      "+-------+------------------+---------+\n",
      "|  count|               100|      100|\n",
      "|   mean|               3.0|     NULL|\n",
      "| stddev|1.4213381090374029|     NULL|\n",
      "|    min|                 1|    Alice|\n",
      "|    max|                 5|     Evan|\n",
      "+-------+------------------+---------+\n",
      "\n",
      "root\n",
      " |-- transaction_id: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-------+------------------+----------------+------------------+\n",
      "|summary|    transaction_id|         user_id|            amount|\n",
      "+-------+------------------+----------------+------------------+\n",
      "|  count|               100|             100|               100|\n",
      "|   mean|              49.5|            3.21|            257.69|\n",
      "| stddev|29.011491975882016|1.35806606405706|131.18255419548944|\n",
      "|    min|                 0|               1|                11|\n",
      "|    max|                99|               5|               496|\n",
      "+-------+------------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also do this \n",
    "users_df.printSchema()\n",
    "users_df.describe().show()\n",
    "\n",
    "transactions_df.printSchema()\n",
    "transactions_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228aa6dc",
   "metadata": {},
   "source": [
    "cartisina join....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35cd50f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+------+\n",
      "|user_id|user_name|transaction_id|amount|\n",
      "+-------+---------+--------------+------+\n",
      "|      1|    Alice|             5|   306|\n",
      "|      1|    Alice|             6|   469|\n",
      "|      1|    Alice|            13|   444|\n",
      "|      1|    Alice|            16|   227|\n",
      "|      1|    Alice|            19|    22|\n",
      "|      1|    Alice|            21|   447|\n",
      "|      1|    Alice|            25|   400|\n",
      "|      1|    Alice|            52|   286|\n",
      "|      1|    Alice|            62|    88|\n",
      "|      1|    Alice|            66|   221|\n",
      "|      1|    Alice|            68|    31|\n",
      "|      1|    Alice|            69|   215|\n",
      "|      1|    Alice|            74|   434|\n",
      "|      1|    Alice|            82|    54|\n",
      "|      1|    Alice|            94|   349|\n",
      "|      1|    Alice|            97|   308|\n",
      "|      1|    Alice|             5|   306|\n",
      "|      1|    Alice|             6|   469|\n",
      "|      1|    Alice|            13|   444|\n",
      "|      1|    Alice|            16|   227|\n",
      "+-------+---------+--------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c3823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|      5|   20|\n",
      "|      1|   20|\n",
      "|      3|   20|\n",
      "|      2|   20|\n",
      "|      4|   20|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check for duplicate keys in users\n",
    "dedup_user = users_df.groupBy('user_id').count().filter(col('count') > 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5198d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|      5|   21|\n",
      "|      3|   24|\n",
      "|      4|   25|\n",
      "|      2|   14|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Check for duplicate keys in transactions\n",
    "dedup_tran = transactions_df.groupBy('user_id').count().filter(col('user_id') > 1 ).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0246923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------+------+\n",
      "|user_id|user_name|transaction_id|amount|\n",
      "+-------+---------+--------------+------+\n",
      "|      5|     Evan|             1|   394|\n",
      "|      5|     Evan|             4|   372|\n",
      "|      1|    Alice|             5|   306|\n",
      "|      1|    Alice|             6|   469|\n",
      "|      3|  Charlie|             0|   147|\n",
      "|      3|  Charlie|             3|   131|\n",
      "|      4|    Diana|             2|   162|\n",
      "|      4|    Diana|             7|   134|\n",
      "|      5|     Evan|             8|    49|\n",
      "|      5|     Evan|            10|   209|\n",
      "|      5|     Evan|            11|   225|\n",
      "|      1|    Alice|            13|   444|\n",
      "|      3|  Charlie|            14|   170|\n",
      "|      2|      Bob|             9|   186|\n",
      "|      2|      Bob|            12|    23|\n",
      "|      2|      Bob|            15|   286|\n",
      "|      1|    Alice|            16|   227|\n",
      "|      1|    Alice|            19|    22|\n",
      "|      1|    Alice|            21|   447|\n",
      "|      3|  Charlie|            18|   316|\n",
      "+-------+---------+--------------+------+\n",
      "only showing top 20 rows\n",
      "Cleaned join count: 100\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Clean users by deduplicating\n",
    "users_clean_df = users_df.dropDuplicates([\"user_id\"])\n",
    "\n",
    "# Re-run join\n",
    "clean_join_df = users_clean_df.join(transactions_df, on=\"user_id\", how=\"inner\")\n",
    "clean_join_df.show()\n",
    "print(\"Cleaned join count:\", clean_join_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e239561",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "You join a customers DataFrame (1M rows) with an orders DataFrame (5M rows) on customer_id.\n",
    "After the join, your resulting DataFrame has only 300K rows — far fewer than expected.\n",
    "\n",
    "You expected at least 1M (every customer should appear), but most are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c708772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers count: 100\n",
      "Purchases count: 100\n",
      "Joined count: 1800\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"JoinDebugScenario2\").getOrCreate()\n",
    "\n",
    "# ----- Customers DataFrame -----\n",
    "customers_data = [\n",
    "    (1, \"Alice\", \"UK\"),\n",
    "    (2, \"Bob\", \"US\"),\n",
    "    (3, \"Charlie\", \"CA\"),\n",
    "    (4, \"Diana\", \"UK\"),\n",
    "    (5, \"Evan\", \"US\"),\n",
    "]\n",
    "\n",
    "# Simulate data quality issues:\n",
    "# - duplicate customer_ids\n",
    "# - inconsistent formatting (string IDs, trailing spaces)\n",
    "customers_data_extended = []\n",
    "for i in range(100):  # 100 rows total\n",
    "    entry = customers_data[i % 5]\n",
    "    if i % 10 == 0:\n",
    "        # introduce trailing spaces or string type ID\n",
    "        customers_data_extended.append((str(entry[0]) + \" \", entry[1], entry[2]))\n",
    "    else:\n",
    "        customers_data_extended.append(entry)\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data_extended, [\"customer_id\", \"customer_name\", \"country\"])\n",
    "\n",
    "\n",
    "# ----- Purchases DataFrame -----\n",
    "purchases_data = []\n",
    "for i in range(100):\n",
    "    customer_id = str((i % 5) + 1)  # also string, to simulate schema mismatch\n",
    "    purchases_data.append((i, customer_id, round(20 + i * 0.75, 2)))\n",
    "\n",
    "purchases_df = spark.createDataFrame(purchases_data, [\"purchase_id\", \"customer_id\", \"amount\"])\n",
    "\n",
    "# ----- Perform join -----\n",
    "joined_df = customers_df.join(purchases_df, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "print(\"Customers count:\", customers_df.count())\n",
    "print(\"Purchases count:\", purchases_df.count())\n",
    "print(\"Joined count:\", joined_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0170d933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- purchase_id: long (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first check the schema \n",
    "customers_df.printSchema()\n",
    "purchases_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b54570a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|          3|   20|\n",
      "|          5|   20|\n",
      "|         1 |   10|\n",
      "|          1|   10|\n",
      "|          4|   20|\n",
      "|          2|   20|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check duplicate in customers_df\n",
    "\n",
    "dedup_cust = customers_df.groupBy('customer_id').count().filter(col('count') > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222aa217",
   "metadata": {},
   "source": [
    "since both customer_id is string and it has spaces...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03d6e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|       customer_id|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|               3.0|\n",
      "| stddev|1.4213381090374029|\n",
      "|    min|                 1|\n",
      "|    max|                 5|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe customer_id distribution in purchases\n",
    "purchases_df.describe([\"customer_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8672ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing data: trim strings, cast IDs to int, deduplicate\n",
    "customers_clean = (\n",
    "    customers_df.withColumn(\"customer_id\", F.trim(F.col(\"customer_id\")).cast(IntegerType()))\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    ")\n",
    "purchases_clean = (\n",
    "    purchases_df.withColumn(\"customer_id\", F.trim(F.col(\"customer_id\")).cast(IntegerType()))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dd329f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "clean_df_joined = customers_clean.join(purchases_clean , 'customer_id', 'inner')\n",
    "print(clean_df_joined.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9997e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
